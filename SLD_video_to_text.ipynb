{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu0AwGFyZGVEvlU4NkTo1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bandi-Lavanya/Indian_Sign_Language_Detection/blob/main/SLD_video_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision transformers opencv-python numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7MnwNMMYNfH",
        "outputId": "2283d512-0b1b-4fe8-8b6b-70e7da8985c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load the pre-trained Indian Sign Language recognition model\n",
        "sign_language_model = pipeline(\"image-classification\", model=\"hemg/Indian-sign-language-classification\")\n",
        "\n",
        "def recognize_sign_from_frame(frame):\n",
        "    # Convert the frame from a numpy array to a PIL Image\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert from BGR to RGB\n",
        "\n",
        "    # Now call the pipeline with the PIL image\n",
        "    result = sign_language_model(pil_image)\n",
        "    return result[0]['label']  # This gives the predicted sign\n",
        "\n",
        "# Example: Read a frame and recognize the sign\n",
        "frame = cv2.imread('/content/sign1.jpeg')  # Read an image (frame)\n",
        "sign = recognize_sign_from_frame(frame)\n",
        "print(f\"Recognized sign: {sign}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5PP-4IYVjg",
        "outputId": "9ff5b55f-c301-4670-fdec-8e5d131e56f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized sign: C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video\n",
        "    sign_sequence = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Process every frame or frame at a consistent interval (e.g., 1 per second)\n",
        "        frame_position = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "        if int(frame_position) % int(frame_rate) == 0:  # Process every nth frame based on FPS\n",
        "            sign = recognize_sign_from_frame(frame)\n",
        "            print(f\"Frame {int(frame_position)}: Recognized sign = {sign}\")  # Print frame and sign\n",
        "            sign_sequence.append(sign)\n",
        "\n",
        "    cap.release()\n",
        "    return sign_sequence\n",
        "\n",
        "\n",
        "# Test with a video path\n",
        "video_path = '/content/WIN_20250215_22_10_36_Pro.mp4'\n",
        "signs = process_video(video_path)\n",
        "print(f\"Detected sign sequence: {signs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imRw4W82YZeh",
        "outputId": "e0447e84-3d85-4b8a-c0e6-72ea1c33a54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 30: Recognized sign = T\n",
            "Frame 60: Recognized sign = T\n",
            "Frame 90: Recognized sign = T\n",
            "Frame 120: Recognized sign = T\n",
            "Detected sign sequence: ['T', 'T', 'T', 'T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the T5 model for text generation\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def generate_text_from_sign_sequence(sign_sequence):\n",
        "    # Join the sequence of signs into a single string (for the model)\n",
        "    sign_sequence_text = \" \".join(sign_sequence)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer.encode(\"translate sign language to text: \" + sign_sequence_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate output text\n",
        "    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode and return the output text\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return result\n",
        "\n",
        "# Example\n",
        "sign_sequence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "sentence = generate_text_from_sign_sequence(sign_sequence)\n",
        "print(f\"Generated sentence: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J05CrZQhZgt-",
        "outputId": "07b5b541-f4e7-4db3-8a92-670904611c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence: sign language to text: hello how are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def generate_text_from_sign_sequence_gpt2(sign_sequence):\n",
        "    sign_sequence_text = \" \".join(sign_sequence)\n",
        "    inputs = tokenizer.encode(sign_sequence_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return result\n",
        "\n",
        "# Example\n",
        "sign_sequence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "sentence = generate_text_from_sign_sequence_gpt2(sign_sequence)\n",
        "print(f\"Generated sentence: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYUMs-26ZouZ",
        "outputId": "31dfc06d-99c6-4e5d-ebdf-6b7bd929172d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence: hello how are you doing?\"\n",
            "\n",
            "\"I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I\n"
          ]
        }
      ]
    }
  ]
}